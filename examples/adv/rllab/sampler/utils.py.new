import numpy as np
from rllab.misc import tensor_utils
import time

def rollout(env, pro_agent, max_path_length=np.inf, animated=False, speedup=1, adv_agent=None):
    observations = []
    pro_actions = []
    if adv_agent: adv_actions = []
    rewards = []
    protagonist_rewards = []
    adversary_rewards = []
    rewards_a = []
    rewards_p = []
    dones_a = []
    pro_agent_infos = []
    if adv_agent: adv_agent_infos = []
    env_infos = []
    o = env.reset()
    pro_agent.reset()
    if adv_agent: adv_agent.reset()
    path_length = 0
    if animated:
        env.render()
    while path_length < max_path_length:
        pro_a, pro_agent_info = pro_agent.get_action(o)
        if adv_agent:
            S = env.wrapped_env.env.get_state()
            adv_a, adv_agent_info = adv_agent.get_action(o)
            class temp_action(object): pro=None; adv=None;
            #from IPython import embed;embed()
            cum_a = temp_action()
            cum_a.pro = pro_a; cum_a.adv = adv_a;

            next_o_a, r_a, d_a, env_info_a = env.step(cum_a)
            pro_actions.append(env.pro_action_space.flatten(pro_a))
            adv_actions.append(env.adv_action_space.flatten(adv_a))
            adv_agent_infos.append(adv_agent_info)

            env.wrapped_env.env.set_state(S[0], S[1])
            cum_a.adv = adv_a*0.0
            next_o, r_p, d, env_info = env.step(cum_a)

            rewards_p.append(r_p)
            rewards_a.append(r_a)
            dones_a.append(d_a)

            adversary_reward = (float(d_a))*10.0
            protagonist_reward = r_p - adversary_reward
            rewards.append(protagonist_reward)
            protagonist_rewards.append(protagonist_reward)
            adversary_rewards.append(adversary_reward)
        else:
            next_o, r, d, env_info = env.step(pro_a)
            pro_actions.append(env.action_space.flatten(pro_a))
            rewards.append(r)

        observations.append(env.observation_space.flatten(o))
        pro_agent_infos.append(pro_agent_info)
        env_infos.append(env_info)
        path_length += 1
        if d:
            break
        o = next_o
        if animated:
            env.render()
            timestep = 0.05
            time.sleep(timestep / speedup)
    #if animated:
    #    return
    if adv_agent:
        return dict(
            observations=tensor_utils.stack_tensor_list(observations),
            pro_actions=tensor_utils.stack_tensor_list(pro_actions),
            adv_actions=tensor_utils.stack_tensor_list(adv_actions),
            rewards=tensor_utils.stack_tensor_list(rewards),
            protagonist_rewards=tensor_utils.stack_tensor_list(protagonist_rewards),
            adversary_rewards=tensor_utils.stack_tensor_list(adversary_rewards),
            done_a=tensor_utils.stack_tensor_list(dones_a),
            pro_agent_infos=tensor_utils.stack_tensor_dict_list(pro_agent_infos),
            adv_agent_infos=tensor_utils.stack_tensor_dict_list(adv_agent_infos),
            env_infos=tensor_utils.stack_tensor_dict_list(env_infos),
        )
    else:
        return dict(
            observations=tensor_utils.stack_tensor_list(observations),
            actions=tensor_utils.stack_tensor_list(pro_actions),
            rewards=tensor_utils.stack_tensor_list(rewards),
            agent_infos=tensor_utils.stack_tensor_dict_list(pro_agent_infos),
            env_infos=tensor_utils.stack_tensor_dict_list(env_infos),
        )
